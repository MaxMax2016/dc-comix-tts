# This config contains the default values for training VITS model on LJSpeech dataset.
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: VITS_GST

batch_size: ??
num_workers: ??
ngpu: ??

train_dataset: "data/train_manifest.json"
validation_datasets: "data/valid_manifest.json"
sup_data_path: "sup_data/"
sup_data_types: [ "speaker_id"]

whitelist_path: "sup_data/text/whitelist/lj_speech.tsv"
phoneme_dict_path: "sup_data/text/cmudict-0.7b_nv22.10"
heteronyms_path: "sup_data/text/heteronyms-052722"

# Default values from librosa.pyin
pitch_fmin: 65.40639132514966
pitch_fmax: 2093.004522404789

sample_rate: 24000
n_mel_channels: 80
n_window_size: 1024
n_window_stride: 256
n_fft: 1024
lowfreq: 0
highfreq: null
window: hann

codec_model: "encodec"


model:
  pitch_fmin: ${pitch_fmin}
  pitch_fmax: ${pitch_fmax}

  sample_rate: ${sample_rate}
  n_mel_channels: ${n_mel_channels}
  n_window_size: ${n_window_size}
  n_window_stride: ${n_window_stride}
  n_fft: ${n_fft}
  lowfreq: ${lowfreq}
  highfreq: ${highfreq}
  window: ${window}
  mel_fmin: 0.0
  mel_fmax: null

  n_speakers: ??
  segment_size: 8192
  c_mel: 45
  c_kl: 1.
  use_spectral_norm: false

  text_normalizer:
    _target_: nemo_text_processing.text_normalization.normalize.Normalizer
    lang: en
    input_case: cased
    whitelist: ${whitelist_path}

  text_normalizer_call_kwargs:
    verbose: false
    punct_pre_process: true
    punct_post_process: true

  text_tokenizer:
    _target_: nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer
    punct: true
    apostrophe: true
    pad_with_space: false
    g2p:
      _target_: nemo_text_processing.g2p.modules.IPAG2P
      phoneme_dict: ${phoneme_dict_path}
      heteronyms: ${heteronyms_path}
      phoneme_probability: 0.8
      # Relies on the heteronyms list for anything that needs to be disambiguated
      ignore_ambiguous_words: false
      use_chars: true
      use_stresses: true

  train_ds:
    dataset:
      _target_: torchdata.data_total.ExtensiveTTSDataset
      manifest_filepath: ${train_dataset}
      sample_rate: ${model.sample_rate}
      sup_data_path: ${sup_data_path}
      sup_data_types: ${sup_data_types}
      n_fft: ${model.n_fft}
      win_length: ${model.n_window_size}
      hop_length: ${model.n_window_stride}
      window: ${model.window}
      n_mels: ${model.n_mel_channels}
      lowfreq: ${model.lowfreq}
      highfreq: ${model.highfreq}
      max_duration: null
      min_duration: 0.7
      ignore_file: null
      trim: False
      pitch_fmin: ${model.pitch_fmin}
      pitch_fmax: ${model.pitch_fmax}
      codec_model: ${codec_model}

    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      pin_memory: true

  validation_ds:
    dataset:
      _target_: torchdata.data_total.ExtensiveTTSDataset
      manifest_filepath: ${validation_datasets}
      sample_rate: ${model.sample_rate}
      sup_data_path: ${sup_data_path}
      sup_data_types: ${sup_data_types}
      n_fft: ${model.n_fft}
      win_length: ${model.n_window_size}
      hop_length: ${model.n_window_stride}
      window: ${model.window}
      n_mels: ${model.n_mel_channels}
      lowfreq: ${model.lowfreq}
      highfreq: ${model.highfreq}
      max_duration: null
      min_duration: 0.7
      ignore_file: null
      trim: False
      pitch_fmin: ${model.pitch_fmin}
      pitch_fmax: ${model.pitch_fmax}
      codec_model: ${codec_model}

    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: ${batch_size}
      num_workers: ${num_workers}
      pin_memory: false

  preprocessor:
    _target_: module.preprocessor.FilterbankFeatures
    nfilt: ${model.n_mel_channels}
    highfreq: ${model.highfreq}
    log: true
    log_zero_guard_type: clamp
    log_zero_guard_value: 1e-05
    lowfreq: ${model.lowfreq}
    n_fft: ${model.n_fft}
    n_window_size: ${model.n_window_size}
    n_window_stride: ${model.n_window_stride}
    pad_to: 1
    pad_value: 0
    sample_rate: ${model.sample_rate}
    window: ${model.window}
    normalize: null
    preemph: null
    dither: 0.0
    frame_splicing: 1
    stft_conv: false
    nb_augmentation_prob : 0
    mag_power: 1.0
    exact_pad: true
    use_grads: true

  synthesizer:
    _target_: module.ref_vits_module.RefSynthesizerTrn
    inter_channels: 192
    hidden_channels: 192
    filter_channels: 768
    n_heads: 2
    n_layers: 6
    kernel_size: 3
    p_dropout: 0.1
    resblock: "1"
    resblock_kernel_sizes: [3,7,11]
    resblock_dilation_sizes: [[1,3,5], [1,3,5], [1,3,5]]
    upsample_rates: [8,8,2,2]
    upsample_initial_channel: 512
    upsample_kernel_sizes: [16,16,4,4]
    n_speakers: ${model.n_speakers}
    gin_channels: 256 # for multi-speaker
    ref_encoder:
      _target_: module.ref_gst.GlobalStyleToken
      cnn_filters: [ 32, 32, 64, 64, 128, 128 ]
      dropout: 0.2
      gru_hidden: ${model.synthesizer.gin_channels}
      gst_size: ${model.synthesizer.gin_channels}
      initial_dim: 513
      n_style_token: 10
      n_style_attn_head: 4

  optim:
    _target_: torch.optim.AdamW
    lr: 2e-4
    betas: [0.9, 0.99]
    eps: 1e-9

    sched:
      name: ExponentialLR
      lr_decay: 0.999875

trainer:
  num_nodes: 1
  devices: ${ngpu}
  accelerator: gpu
  strategy: ddp
  precision: 16
  # amp_backend: 'apex'
  # amp_level: 'O2'
  # benchmark: true
  max_epochs: 1000
  accumulate_grad_batches: 1
  enable_checkpointing: false # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 50
  check_val_every_n_epoch: 1

exp_manager:
  exp_dir: exp_VITS
  name: ${name}
  create_tensorboard_logger: false
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: loss_gen_all
    mode: min
  create_wandb_logger: true
  wandb_logger_kwargs:
    name: ${name}
    project: RefVits
    entity: null
  resume_if_exists: true
  resume_ignore_no_checkpoint: true